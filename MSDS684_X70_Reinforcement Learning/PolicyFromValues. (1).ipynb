{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Derivation from Value Functions\n",
    "\n",
    "This notebook demonstrates how to derive an optimal policy directly from the state-value function (V(s)) in a grid world environment. We start with an arbitrary policy, evaluate it to find its value function, and then improve the policy based on those values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import necessary functions, create the standard grid world environment, and define an initial, arbitrary policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Input Policy:\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |  Right |  Right |     Up |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from rlgridworld.standard_grid import create_standard_grid\n",
    "from rlgridworld.algorithms import iterative_policy_evaluation\n",
    "\n",
    "gw = create_standard_grid()\n",
    "\n",
    "# Initial arbitrary policy\n",
    "policy = { \n",
    "    (0,0):'up', (0,1):'right',(0,2):'right',(0,3):'up',\n",
    "    (1,0):'up', (1,1):'', (1,2):'right', (1,3):'',\n",
    "    (2,0):'right', (2,1):'right', (2,2):'right', (2,3):''\n",
    "}\n",
    "\n",
    "print(\"Initial Input Policy:\")\n",
    "gw.print_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Policy Evaluation (Gamma = 0.9)\n",
    "\n",
    "Perform iterative policy evaluation using the initial policy and a discount factor `gamma = 0.9` to compute the state-value function V(s) for this policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values for the initial policy (gamma=0.9):\n",
      "-------------------------------------\n",
      "|   0.81 |   0.90 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.73 |   0.00 |  -1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.66 |  -0.81 |  -0.90 |  -1.00 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iterative_policy_evaluation(gw, policy, gamma = 0.9)\n",
    "\n",
    "print(\"\\nValues for the initial policy (gamma=0.9):\")\n",
    "gw.print_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Function to Compute Policy from Values\n",
    "\n",
    "Define a function `compute_policy_from_values` that takes the grid world and its current value function as input. For each state, it iterates through possible actions, calculates the expected return for taking each action (using the Bellman equation component: `reward + gamma * value_at_destination`), and selects the action that yields the highest expected return. This creates a new, improved policy based on the current values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_from_values(gw, gamma = 0.9):\n",
    "    \"\"\"Computes the greedy policy based on the current values in the grid world.\"\"\"\n",
    "    # create null policy dictionary\n",
    "    new_policy = {}\n",
    "\n",
    "    # loop over all states\n",
    "    for i in range(gw.M):\n",
    "        for j in range(gw.N):\n",
    "            state = (i,j)\n",
    "\n",
    "            # assign 'no' policy to barrier states, there are no actions at barrier states\n",
    "            if gw.is_barrier(state):\n",
    "                new_policy[state] = ''\n",
    "                continue # Skip to next state\n",
    "\n",
    "            # assign 'no' policy to terminal sttes, there are no actions at terminal states \n",
    "            if gw.is_terminal(state):\n",
    "                new_policy[state] = ''\n",
    "                continue # Skip to next state\n",
    "\n",
    "            # for all non terminal and non barrier states\n",
    "            # set candidate best action and best value\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "\n",
    "            # get dictionary of all valid decisions and rewards at current state (i,j)\n",
    "            dr = gw.valid_decisions_and_rewards(state)\n",
    "\n",
    "            # iterate over all action, reward in \n",
    "            for action, reward in dr.items():\n",
    "                # get reward for current action (Note: reward in dr.items() might be outdated, recalculate)\n",
    "                current_reward = gw.get_reward_for_action(state,action)\n",
    "                \n",
    "                # get the value of the destination state for the current action\n",
    "                value_at_dest = gw.get_value_at_destination(state,action)\n",
    "\n",
    "                # compute candidate value (expected return for taking this action from this state)\n",
    "                value = current_reward + gamma*value_at_dest\n",
    "\n",
    "                # if value is better, then update best action and best value\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "\n",
    "            # add best action to the policy dictionary \n",
    "            new_policy[state] = best_action\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Derive and Compare Policies (Based on Gamma = 0.9 Values)\n",
    "\n",
    "Use the `compute_policy_from_values` function (with the default `gamma=0.9`) to derive a new policy based on the values calculated in step 2. Compare this new policy to the original arbitrary policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Policy:\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |  Right |  Right |     Up |\n",
      "-------------------------------------\n",
      "\n",
      "New Policy (derived from gamma=0.9 values):\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |     Up |        |\n",
      "-------------------------------------\n",
      "|     Up |   Left |   Left |   Left |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compute the policy derived from the V(s) calculated with gamma=0.9\n",
    "# Note: The function uses gamma=0.9 by default, matching the values just calculated.\n",
    "new_policy_g09 = compute_policy_from_values(gw, gamma=0.9) \n",
    "\n",
    "print(\"Original Policy:\")\n",
    "gw.print_policy(policy)\n",
    "\n",
    "print(\"\\nNew Policy (derived from gamma=0.9 values):\")\n",
    "gw.print_policy(new_policy_g09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Policy Evaluation with Different Gamma (Gamma = 0.8)\n",
    "\n",
    "Now, let's re-evaluate the *original* arbitrary policy, but this time using a different discount factor, `gamma = 0.8`. This will result in a different state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values for the initial policy (gamma=0.8):\n",
      "-------------------------------------\n",
      "|   0.64 |   0.80 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.51 |   0.00 |  -1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.41 |  -0.64 |  -0.80 |  -1.00 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate the original policy with gamma = 0.8\n",
    "iterative_policy_evaluation(gw, policy, gamma = 0.8)\n",
    "\n",
    "print(\"\\nValues for the initial policy (gamma=0.8):\")\n",
    "gw.print_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Derive and Compare Policies (Based on Gamma = 0.8 Values)\n",
    "\n",
    "Derive another new policy, this time using the values calculated with `gamma = 0.8`. Make sure to pass `gamma=0.8` to the `compute_policy_from_values` function. Compare this policy to the original one and the one derived using `gamma = 0.9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Policy:\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |  Right |  Right |     Up |\n",
      "-------------------------------------\n",
      "\n",
      "New Policy (derived from gamma=0.9 values):\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |     Up |        |\n",
      "-------------------------------------\n",
      "|     Up |   Left |   Left |   Left |\n",
      "-------------------------------------\n",
      "\n",
      "New Policy (derived from gamma=0.8 values):\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |     Up |        |\n",
      "-------------------------------------\n",
      "|     Up |   Left |   Left |   Left |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compute the policy derived from the V(s) calculated with gamma=0.8\n",
    "new_policy_g08 = compute_policy_from_values(gw, gamma=0.8)\n",
    "\n",
    "print(\"Original Policy:\")\n",
    "gw.print_policy(policy)\n",
    "\n",
    "print(\"\\nNew Policy (derived from gamma=0.9 values):\")\n",
    "gw.print_policy(new_policy_g09)\n",
    "\n",
    "print(\"\\nNew Policy (derived from gamma=0.8 values):\")\n",
    "gw.print_policy(new_policy_g08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Further Experimentation\n",
    "\n",
    "You can experiment further by:\n",
    "*   Trying different values for `gamma` (e.g., 1.0, 0.5, 0.1) and observing how the resulting values and derived policies change.\n",
    "*   Starting with a different initial policy.\n",
    "*   Modifying the grid world rewards or structure (using functions from `rlgridworld`).\n",
    "*   Visualizing the value function or policy (e.g., using heatmaps or arrows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Evaluate and Derive Policy for Gamma = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Values for the initial policy (gamma=1.0):\n",
      "-------------------------------------\n",
      "|   1.00 |   1.00 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   1.00 |   0.00 |  -1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   1.00 |  -1.00 |  -1.00 |  -1.00 |\n",
      "-------------------------------------\n",
      "\n",
      "New Policy (derived from gamma=1.0 values):\n",
      "-------------------------------------\n",
      "|  Right |   Left |   Left |        |\n",
      "-------------------------------------\n",
      "|   Down |        |     Up |        |\n",
      "-------------------------------------\n",
      "|     Up |   Left |   Left |   Left |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate original policy with gamma = 1.0\n",
    "iterative_policy_evaluation(gw, policy, gamma = 1.0)\n",
    "print(\"\\nValues for the initial policy (gamma=1.0):\")\n",
    "gw.print_values()\n",
    "\n",
    "# Compute the policy derived from the V(s) calculated with gamma=1.0\n",
    "new_policy_g10 = compute_policy_from_values(gw, gamma=1.0)\n",
    "\n",
    "print(\"\\nNew Policy (derived from gamma=1.0 values):\")\n",
    "gw.print_policy(new_policy_g10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

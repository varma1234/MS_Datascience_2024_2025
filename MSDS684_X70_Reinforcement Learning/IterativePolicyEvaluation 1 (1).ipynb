{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Policy Evaluation\n",
    "\n",
    "This notebook demonstrates the Iterative Policy Evaluation algorithm, used to estimate the state-value function V(s) for a given policy in a Reinforcement Learning environment.\n",
    "\n",
    "Reference: Sutton and Barto, Reinforcement Learning 2nd. Edition, page 75.\n",
    "\n",
    "![Iterative Policy Evaluation Algorithm](./figures/IterativePolicyEvaluation.png)\n",
    "*Figure: Pseudocode for Iterative Policy Evaluation from Sutton and Barto.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept\n",
    "\n",
    "A **policy** dictates the action to take in every state. Iterative Policy Evaluation aims to compute the value (expected return) of being in each state, assuming the agent follows the given policy.\n",
    "\n",
    "The core idea is to repeatedly sweep through all states and update their values based on the expected return, which considers the immediate reward and the discounted value of the next state according to the policy. This process continues until the values converge (i.e., the changes in values between iterations become very small).\n",
    "\n",
    "**Algorithm Outline:**\n",
    "```\n",
    "Initialize V(s) arbitrarily (e.g., V(s)=0 for all non-terminal states)\n",
    "Loop until convergence:\n",
    "  Δ ← 0\n",
    "  Loop for each state s:\n",
    "    v ← V(s)  # Store old value\n",
    "    # Update rule based on the expected value under the policy π\n",
    "    V(s) ← Σ [ P(s',r|s,π(s)) * (r + γ * V(s')) ] \n",
    "    # Simplified for deterministic policy/environment:\n",
    "    # V(s) ← reward(s, π(s)) + γ * V(next_state(s, π(s)))\n",
    "    Δ ← max(Δ, |v - V(s)|)\n",
    "  If Δ < θ (a small threshold): break loop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Grid World Environment\n",
    "\n",
    "Import necessary libraries and create the standard grid world environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlgridworld.standard_grid import create_standard_grid\n",
    "\n",
    "gw = create_standard_grid()\n",
    "\n",
    "# Optional: Modify rewards if needed (example commented out)\n",
    "#gw.set_reward((0,0), \"up\", -2) \n",
    "#gw.set_reward((0,1), \"right\", -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Policy (π)\n",
    "\n",
    "Specify the policy we want to evaluate. This is a dictionary mapping states `(row, col)` to actions (`'up'`, `'down'`, `'left'`, `'right'`). Empty strings (`''`) denote terminal or barrier states where no action is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = { \n",
    "    (0,0):'up', (0,1):'right',(0,2):'right',(0,3):'up',\n",
    "    (1,0):'up', (1,1):'', (1,2):'right', (1,3):'',\n",
    "    (2,0):'right', (2,1):'right', (2,2):'right', (2,3):''\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial State\n",
    "\n",
    "Print the policy and the initial values (usually zero) before evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy to Evaluate:\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |  Right |  Right |     Up |\n",
      "-------------------------------------\n",
      "\n",
      "Initial Values (Before Evaluation):\n",
      "-------------------------------------\n",
      "|   0.00 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.00 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.00 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy to Evaluate:\")\n",
    "gw.print_policy(policy)\n",
    "\n",
    "print(\"\\nInitial Values (Before Evaluation):\")\n",
    "gw.print_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Iterative Policy Evaluation Implementation\n",
    "\n",
    "The function `iterative_policy_evaluation` implements the algorithm described above.\n",
    "\n",
    "- It repeatedly loops through all states (`while True` loop, breaks when converged).\n",
    "- For each non-terminal, non-barrier state:\n",
    "    - It gets the current value `old_value`.\n",
    "    - It finds the action prescribed by the `policy` for that state.\n",
    "    - It calculates the expected `new_value` using the Bellman equation for V(s) under the policy: `new_value = reward + gamma * value_at_dest`.\n",
    "    - It updates the state's value in the grid world `gw.set_value(state, new_value)`.\n",
    "    - It keeps track of the `biggest_change` in values during the sweep.\n",
    "- The outer loop terminates (`break`) when the `biggest_change` across all states in a sweep is less than a small threshold `theta`, indicating convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(gw, policy, gamma=0.9, theta=0.001):\n",
    "    \"\"\"Performs iterative policy evaluation to find V(s) for a given policy.\"\"\"\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        # Loop through each state in the grid\n",
    "        for node in gw:\n",
    "            state = node.state\n",
    "            # Only evaluate non-terminal and non-barrier states\n",
    "            if not gw.is_terminal(state) and not gw.is_barrier(state):\n",
    "                # Get current (old) value\n",
    "                old_value = gw.get_value(state)\n",
    "                \n",
    "                # Get action from the fixed policy\n",
    "                action = policy[state]\n",
    "                \n",
    "                # Get immediate reward for taking that action from this state\n",
    "                reward = gw.get_reward_for_action(state, action)\n",
    "                \n",
    "                # Get the value of the state we'd land in (destination)\n",
    "                value_at_dest = gw.get_value_at_destination(state, action)\n",
    "                \n",
    "                # Compute the updated value using the Bellman expectation equation\n",
    "                new_value = reward + gamma * value_at_dest\n",
    "                \n",
    "                # Update the value function for the state\n",
    "                gw.set_value(state, new_value)\n",
    "                \n",
    "                # Track the maximum change in value across all states\n",
    "                biggest_change = max(\n",
    "                    biggest_change, abs(new_value - old_value))\n",
    "                    \n",
    "        # Check for convergence after iterating over all states\n",
    "        if biggest_change < theta:\n",
    "            break # Values have converged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation (Gamma = 0.9)\n",
    "\n",
    "Execute the iterative policy evaluation function with a discount factor `gamma = 0.9` and print the resulting converged values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |  Right |  Right |     Up |\n",
      "-------------------------------------\n",
      "\n",
      "Converged Values for the policy (gamma=0.9):\n",
      "-------------------------------------\n",
      "|   0.81 |   0.90 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.73 |   0.00 |  -1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.66 |  -0.81 |  -0.90 |  -1.00 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy:\")\n",
    "gw.print_policy(policy)\n",
    "\n",
    "# Run the evaluation\n",
    "iterative_policy_evaluation(gw, policy, gamma = 0.9)\n",
    "\n",
    "print(\"\\nConverged Values for the policy (gamma=0.9):\")\n",
    "gw.print_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation (Gamma = 0.8)\n",
    "\n",
    "Repeat the evaluation, but this time with a different discount factor `gamma = 0.8`. Note that this requires resetting the values first or running on a fresh grid object if you want to compare accurately, as the `iterative_policy_evaluation` modifies the `gw` object in place. For simplicity here, we run it on the same `gw`, overwriting the previous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |  Right |  Right |     Up |\n",
      "-------------------------------------\n",
      "\n",
      "Converged Values for the policy (gamma=0.8):\n",
      "-------------------------------------\n",
      "|   0.64 |   0.80 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.51 |   0.00 |  -1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.41 |  -0.64 |  -0.80 |  -1.00 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy:\")\n",
    "gw.print_policy(policy)\n",
    "\n",
    "iterative_policy_evaluation(gw, policy, gamma = 0.8)\n",
    "\n",
    "print(\"\\nConverged Values for the policy (gamma=0.8):\")\n",
    "gw.print_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

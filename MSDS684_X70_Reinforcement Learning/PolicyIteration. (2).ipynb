{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sutton and Barto, Reinforcement Learning 2nd. Edition, page 80.\n",
    "\n",
    "![Sutton and Barto, Reinforcement Learning 2nd. Edition.](./Figures/PolicyIteration.png)\n",
    "Policy Iteration for estimating Ï€\n",
    "\n",
    "\n",
    "\n",
    "**Policy Iteration**\n",
    "\n",
    "\n",
    "\n",
    "In iterative policy evaluation the values for a fixed policy are used.  Once these values have been determined we can then examine the rewards and values at destination states to determine if there is a better policy.  *Policy Iteration* is the resulting algorithm. \n",
    "\n",
    "\n",
    "\n",
    "This calculation is repeated until the policy does not change.\n",
    "\n",
    "\n",
    "\n",
    "**Policy Iteration Algorithm**\n",
    "\n",
    "```python\n",
    "\n",
    "Given a policy\n",
    "\n",
    "while not converged:\n",
    "\n",
    "    compute values using iterative policy evaluation\n",
    "\n",
    "    compute new policy from values\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries\n",
    "\n",
    "Import functions required for creating the grid world environment and implementing the policy iteration algorithm components (policy evaluation and policy improvement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlgridworld.standard_grid import create_standard_grid\n",
    "from rlgridworld.algorithms import iterative_policy_evaluation\n",
    "from rlgridworld.algorithms import compute_policy_from_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Grid World Environment\n",
    "\n",
    "Instantiate the standard grid world environment used for the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = create_standard_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Initial Policy\n",
    "\n",
    "Define an initial, arbitrary policy for the agent. This policy maps states (represented by coordinates) to actions ('up', 'down', 'left', 'right'). Empty strings indicate terminal states or states with no defined action in this initial policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = { \n",
    "    (0,0):'right', (0,1):'right',(0,2):'right',(0,3):'up',\n",
    "    (1,0):'up', (1,1):'', (1,2):'right', (1,3):'',\n",
    "    (2,0):'right', (2,1):'right', (2,2):'right', (2,3):''\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Policy Iteration Function\n",
    "\n",
    "Implement the core Policy Iteration algorithm as described in Sutton and Barto (page 80). This function iteratively performs policy evaluation (calculating state values V(s) for the current policy) and policy improvement (updating the policy based on the calculated values) until the policy stabilizes (no longer changes between iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from page 80 of Sutton and Barto, RL, 2nd. Ed.\n",
    "\n",
    "def policy_iteration(gw, policy, gamma=0.9, epsilon=0.001):\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # perform iterative policy evaluation to update values\n",
    "        # This function modifies gw.V internally\n",
    "        iterative_policy_evaluation(gw, policy, gamma, epsilon)\n",
    "\n",
    "        # update policy from new values stored in gw.V\n",
    "        new_policy = compute_policy_from_values(gw, gamma)\n",
    "\n",
    "        # see if policy has changed\n",
    "        policy_stable = True # Assume stable initially\n",
    "        current_states = set(policy.keys())\n",
    "        new_states = set(new_policy.keys())\n",
    "        all_states = current_states.union(new_states)\n",
    "\n",
    "        for state in all_states:\n",
    "            current_action = policy.get(state, '') # Get action or default to ''\n",
    "            new_action = new_policy.get(state, '') # Get action or default to ''\n",
    "            if current_action != new_action:\n",
    "                policy_stable = False\n",
    "                break\n",
    "\n",
    "        # update policy for the next iteration\n",
    "        policy = new_policy\n",
    "\n",
    "        # repeat until policy does not change\n",
    "        if policy_stable:\n",
    "            break\n",
    "    # Return the final stable policy \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "1.  Print the initial policy.\n",
    "2.  Evaluate the initial policy using `iterative_policy_evaluation` just to see the starting values (this step is *not* part of the core policy iteration loop itself, but done here for demonstration).\n",
    "3.  Print the values associated with the initial policy.\n",
    "4.  Execute the `policy_iteration` function to find the optimal policy. We pass a copy of the initial policy to avoid modifying the original dictionary if it's needed elsewhere.\n",
    "5.  The `policy_iteration` function returns the final, stable policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Policy\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |  Right |        |\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |     Up |\n",
      "-------------------------------------\n",
      "\n",
      "Initial Policy Values (before iteration)\n",
      "-------------------------------------\n",
      "|   0.81 |   0.90 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.73 |   0.00 |  -1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|  -0.73 |  -0.81 |  -0.90 |  -1.00 |\n",
      "-------------------------------------\n",
      "\n",
      "Final Policy (after iteration)\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |     Up |        |\n",
      "-------------------------------------\n",
      "|  Right |  Right |     Up |   Left |\n",
      "-------------------------------------\n",
      "\n",
      "Final Policy Values (after iteration)\n",
      "-------------------------------------\n",
      "|   0.81 |   0.90 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.73 |   0.00 |   0.90 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.66 |   0.73 |   0.81 |   0.73 |\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_policy_display = policy.copy()\n",
    "print(\"\")\n",
    "print(\"Initial Policy\")\n",
    "gw.print_policy(initial_policy_display)\n",
    "print(\"\")\n",
    "\n",
    "# note: this execution of iterative policy evaluation is not part \n",
    "# of the policy iteration algorithm.  It is for the purpose of \n",
    "# displaying the values associated with the input policy\n",
    "\n",
    "# Create a temporary grid instance or reset values if needed for display\n",
    "temp_gw_for_initial_values = create_standard_grid() \n",
    "iterative_policy_evaluation(temp_gw_for_initial_values, initial_policy_display) # Use default gamma/epsilon\n",
    "print(\"Initial Policy Values (before iteration)\")\n",
    "temp_gw_for_initial_values.print_values()\n",
    "del temp_gw_for_initial_values # Clean up temporary object\n",
    "\n",
    "# run policy iteration algorithm on the main grid world object 'gw'\n",
    "# Make sure gw's values are reset or start fresh if necessary before this step\n",
    "gw = create_standard_grid() # Re-create to ensure clean state values\n",
    "final_policy = policy_iteration(gw, policy.copy()) # Pass a copy of the original policy\n",
    "# The policy_iteration function modifies the values in gw internally.\n",
    "\n",
    "# print new policy and values\n",
    "print(\"\") \n",
    "print(\"Final Policy (after iteration)\")\n",
    "gw.print_policy(final_policy)\n",
    "print(\"\")\n",
    "print(\"Final Policy Values (after iteration)\")\n",
    "gw.print_values()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Q-Learning Implementation in a Grid World\n",
                "\n",
                "This notebook demonstrates the Q-learning algorithm applied to a simple grid world environment. The implementation is based on the Q-learning algorithm described in Sutton and Barto's \"Reinforcement Learning: An Introduction,\" specifically referencing the content on page 130 of the second edition.\n",
                "\n",
                "The core idea of Q-learning is to learn an optimal policy by estimating the action-value function, denoted as Q(s, a), which represents the expected cumulative reward of taking action 'a' in state 's' and following an optimal policy thereafter. Q-learning is an off-policy temporal difference (TD) control method, meaning it learns the optimal policy independently of the agent's actions. This allows it to learn from exploratory or even random actions, making it a robust algorithm for various reinforcement learning problems.\n",
                "\n",
                "The following image illustrates the Q-learning concept as presented in Sutton and Barto's book:\n",
                "\n",
                "![Sutton and Barto, Reinforcement Learning 2nd. Edition.](./figures/QLearning.png)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "000079e3",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "from rlgridworld.standard_grid import create_standard_grid"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b395e849",
            "metadata": {},
            "source": [
                "### Q-Learning Algorithm Implementation\n",
                "\n",
                "The following code implements the Q-learning algorithm to train an agent to navigate a grid world. The `play_game` function simulates an episode of the game, updating the Q-values based on the agent's experience. Key parameters such as `epsilon` (exploration rate), `gamma` (discount factor), and `alpha` (learning rate) control the learning process. The agent starts at a defined state and continues until it reaches a terminal state, updating its Q-values along the way.\n",
                "\n",
                "**Algorithm Parameters:**\n",
                "*   `epsilon`: The probability of exploration, determining how often the agent takes a random action instead of the greedy action.\n",
                "*   `gamma`: The discount factor, which determines the importance of future rewards. A higher gamma value gives more weight to future rewards.\n",
                "*   `alpha`: The learning rate, which controls how much the Q-values are updated based on new information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "f7fde2cb",
            "metadata": {},
            "outputs": [],
            "source": [
                "def play_game(gw, Q):\n",
                "    epsilon = 0.05 # probability of exploration\n",
                "    gamma = 0.9 # discount factor for future rewards\n",
                "    alpha = 0.1 # Q update fraction\n",
                "    state = (0, 0)\n",
                "    converged = False\n",
                "    while not converged:\n",
                "        action, _ = max_dict(Q[state])\n",
                "        all_actions = gw.valid_decisions(state)\n",
                "        action = random_action(action, all_actions, epsilon)\n",
                "        reward = gw.get_reward_for_action(state, action)\n",
                "        stateprime = move(state, action)\n",
                "        iprime, jprime = stateprime\n",
                "        if not gw.is_terminal(stateprime):\n",
                "            _, destvalue = max_dict(Q[stateprime])\n",
                "            Q[state][action] = Q[state][action] + alpha*(reward + gamma*destvalue - Q[state][action])\n",
                "            state = stateprime\n",
                "        if gw.is_terminal(stateprime):\n",
                "            Q[state][action] = Q[state][action] + alpha*(reward - Q[state][action]) \n",
                "            converged = True\n",
                "    return Q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "6f16f6ab",
            "metadata": {},
            "outputs": [],
            "source": [
                "def move(state, action):\n",
                "    i, j = state\n",
                "    if action == 'left':\n",
                "        j = j-1\n",
                "    if action == 'right':\n",
                "        j = j+1\n",
                "    if action == 'down':\n",
                "        i = i-1\n",
                "    if action == 'up':\n",
                "        i = i+1\n",
                "    return (i, j)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "47191167",
            "metadata": {},
            "outputs": [],
            "source": [
                "def random_action(action, all_actions, epsilon):\n",
                "    p = np.random.random_sample()\n",
                "    if p < (1 - epsilon):\n",
                "        return action\n",
                "    else:\n",
                "        return np.random.choice(all_actions)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "6ded691f",
            "metadata": {},
            "outputs": [],
            "source": [
                "def max_dict(d):\n",
                "    max_key = None\n",
                "    max_val = float('-inf')\n",
                "    for k, v in d.items():\n",
                "        if v > max_val:\n",
                "            max_val = v\n",
                "            max_key = k\n",
                "    return max_key, max_val"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "10336071",
            "metadata": {},
            "outputs": [],
            "source": [
                "def init_Q(gw):\n",
                "    Q = {}\n",
                "    for i in range(0, gw.M):\n",
                "        for j in range(0, gw.N):\n",
                "            state = (i,j)\n",
                "            if not gw.is_barrier(state) and not gw.is_terminal(state):\n",
                "                Q[state] = {}\n",
                "                all_actions = gw.valid_decisions(state)\n",
                "                for a in all_actions:\n",
                "                    Q[state][a] = 0\n",
                "    return Q"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3657e55b",
            "metadata": {},
            "source": [
                "### Grid World Initialization\n",
                "\n",
                "Here, we create the standard grid world environment and initialize the Q-value dictionary. The grid world is a 2D environment with specified start and end states, barriers, and rewards. The Q-value dictionary stores the estimated Q-values for each state-action pair. Initially, all Q-values are set to zero."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "a754a582",
            "metadata": {},
            "outputs": [],
            "source": [
                "gw = create_standard_grid()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "d2be2282",
            "metadata": {},
            "outputs": [],
            "source": [
                "Q = init_Q( gw )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cd86035c",
            "metadata": {},
            "source": [
                "### Initial Q-Values\n",
                "\n",
                "The initial Q-value dictionary contains the Q-values for all possible state-action pairs in the grid world. Each state is represented as a tuple (row, column), and the actions are 'left', 'right', 'up', and 'down'. The Q-values are initially set to 0.0. The tuples at the beginning of each line represent the dictionary keys, and each dictionary value is another dictionary. For the second dictionary, the decisions are the dictionary keys. The values in this dictionary are the Q values for each action at the designated state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "8bfcc6e2",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{(0, 0): {'right': 0, 'up': 0},\n",
                            " (0, 1): {'left': 0, 'right': 0},\n",
                            " (0, 2): {'left': 0, 'right': 0, 'up': 0},\n",
                            " (0, 3): {'left': 0, 'up': 0},\n",
                            " (1, 0): {'down': 0, 'up': 0},\n",
                            " (1, 2): {'right': 0, 'down': 0, 'up': 0},\n",
                            " (2, 0): {'right': 0, 'down': 0},\n",
                            " (2, 1): {'left': 0, 'right': 0},\n",
                            " (2, 2): {'left': 0, 'right': 0, 'down': 0}}"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "Q"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d04aaa87",
            "metadata": {},
            "source": [
                "### Playing the Game\n",
                "\n",
                "The `play_game` function simulates one episode of the game. During each episode, the agent starts at the initial state and takes actions based on the Q-values and the exploration rate (`epsilon`). The Q-values are updated using the Q-learning update rule. This process continues until the agent reaches a terminal state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "1d613d94",
            "metadata": {},
            "outputs": [],
            "source": [
                "Q = play_game(gw, Q)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "98455784",
            "metadata": {},
            "source": [
                "### Updated Q-Values After One Iteration\n",
                "\n",
                "After playing one iteration of the game, the Q-values are updated based on the agent's experience. The `alpha` factor in the Q-value update rule determines how much the Q-values are adjusted. Note the effect of the alpha factor in the updates of the Q values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "7ba5fdf7",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{(0, 0): {'right': 0.0, 'up': 0.0},\n",
                            " (0, 1): {'left': 0.0, 'right': 0.0},\n",
                            " (0, 2): {'left': 0.0, 'right': 0, 'up': 0.0},\n",
                            " (0, 3): {'left': 0, 'up': 0},\n",
                            " (1, 0): {'down': 0.0, 'up': 0},\n",
                            " (1, 2): {'right': -0.1, 'down': 0, 'up': 0},\n",
                            " (2, 0): {'right': 0, 'down': 0},\n",
                            " (2, 1): {'left': 0, 'right': 0},\n",
                            " (2, 2): {'left': 0, 'right': 0, 'down': 0}}"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "Q"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e6d0de5e",
            "metadata": {},
            "source": [
                "### Playing the Game Multiple Times\n",
                "\n",
                "By playing the game multiple times, the agent learns to make better decisions and the Q-values converge towards the optimal values. The following code plays the game for another iteration and displays the updated Q-values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "a5449578",
            "metadata": {},
            "outputs": [],
            "source": [
                "Q = play_game(gw, Q)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "766a3756",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{(0, 0): {'right': 0.0, 'up': 0.0},\n",
                            " (0, 1): {'left': 0.0, 'right': 0.0},\n",
                            " (0, 2): {'left': 0.0, 'right': 0.0, 'up': 0.0},\n",
                            " (0, 3): {'left': 0.0, 'up': -0.1},\n",
                            " (1, 0): {'down': 0.0, 'up': 0.0},\n",
                            " (1, 2): {'right': -0.1, 'down': 0.0, 'up': 0},\n",
                            " (2, 0): {'right': 0.0, 'down': 0.0},\n",
                            " (2, 1): {'left': 0.0, 'right': 0.0},\n",
                            " (2, 2): {'left': 0.0, 'right': 0, 'down': 0}}"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "Q"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "38c69208",
            "metadata": {},
            "source": [
                "### Training the Agent\n",
                "\n",
                "To train the agent effectively, we need to play the game many times. The following code plays the game 10,000 times and updates the Q-values accordingly. This allows the agent to explore the environment and learn the optimal policy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "5438cbfa",
            "metadata": {},
            "outputs": [],
            "source": [
                "for _ in range(10000):\n",
                "    Q = play_game(gw, Q)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "a79ebd80",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{(0, 0): {'right': 0.5314409999982149, 'up': 0.6560999999999979},\n",
                            " (0, 1): {'left': 0.5904899999999977, 'right': 0.3340773402574159},\n",
                            " (0, 2): {'left': 0.5314409951927251,\n",
                            "  'right': 0.024214856660908536,\n",
                            "  'up': 0.043046698752991115},\n",
                            " (0, 3): {'left': 0.162361316099566, 'up': -0.1},\n",
                            " (1, 0): {'down': 0.5904899999997081, 'up': 0.7289999999999983},\n",
                            " (1, 2): {'right': -0.34390000000000004,\n",
                            "  'down': 0.478296751420081,\n",
                            "  'up': 0.3685589999999998},\n",
                            " (2, 0): {'right': 0.8099999999999987, 'down': 0.6560999999998001},\n",
                            " (2, 1): {'left': 0.7289999999983406, 'right': 0.899999999999999},\n",
                            " (2, 2): {'left': 0.8099999974863857,\n",
                            "  'right': 0.9999999999999996,\n",
                            "  'down': 0.4304659933034021}}"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "Q"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9380b3da",
            "metadata": {},
            "source": [
                "### Extracting the Policy\n",
                "\n",
                "After training the agent, we can extract the optimal policy from the Q-values. The policy is a mapping from each state to the best action to take in that state. The following code extracts the policy and value function from the Q-table and prints the policy and values for each state in the grid world."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "179c2f68",
            "metadata": {},
            "outputs": [],
            "source": [
                "policy = {}\n",
                "for i in range(gw.M):\n",
                "    for j in range(gw.N):\n",
                "        state = (i,j)\n",
                "        if gw.is_barrier(state):\n",
                "            policy[state] = ''\n",
                "        if gw.is_terminal(state):\n",
                "            policy[state] = ''\n",
                "        if not gw.is_barrier(state) and not gw.is_terminal(state):\n",
                "            action, value = max_dict(Q[state])\n",
                "            gw.set_value(state, value)\n",
                "            policy[state] = action"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "eda7ffa1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "-------------------------------------\n",
                        "|   0.81 |   0.90 |   1.00 |   0.00 |\n",
                        "-------------------------------------\n",
                        "|   0.73 |   0.00 |   0.48 |   0.00 |\n",
                        "-------------------------------------\n",
                        "|   0.66 |   0.59 |   0.53 |   0.16 |\n",
                        "-------------------------------------\n",
                        "-------------------------------------\n",
                        "|  Right |  Right |  Right |        |\n",
                        "-------------------------------------\n",
                        "|     Up |        |   Down |        |\n",
                        "-------------------------------------\n",
                        "|     Up |   Left |   Left |   Left |\n",
                        "-------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "gw.print_values()\n",
                "gw.print_policy(policy)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "83e239c6",
            "metadata": {},
            "source": [
                "### Visualizing the Learned Policy\n",
                "\n",
                "The following cell visualizes the learned policy on the grid. Arrows indicate the optimal action determined by the Q-learning agent for each state. States without arrows are either terminal states or barriers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2834edca",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Create a figure and axes\n",
                "fig, ax = plt.subplots()\n",
                "\n",
                "# Grid dimensions (assuming M rows, N columns)\n",
                "M = gw.M\n",
                "N = gw.N\n",
                "\n",
                "# Draw grid lines\n",
                "ax.set_xticks(np.arange(-.5, N, 1), minor=True)\n",
                "ax.set_yticks(np.arange(-.5, M, 1), minor=True)\n",
                "ax.grid(which='minor', color='black', linestyle='-', linewidth=1)\n",
                "\n",
                "# Set limits and remove axis labels\n",
                "ax.set_xlim(-0.5, N - 0.5)\n",
                "ax.set_ylim(-0.5, M - 0.5)\n",
                "ax.set_xticklabels([])\n",
                "ax.set_yticklabels([])\n",
                "ax.tick_params(axis='both', which='both', length=0) # Hide ticks\n",
                "\n",
                "# Define arrow properties\n",
                "arrow_props = dict(facecolor='black', width=0.1, head_width=0.4, head_length=0.4)\n",
                "\n",
                "# Plot arrows for the policy\n",
                "for state, action in policy.items():\n",
                "    i, j = state\n",
                "    # Convert grid coords (i, j) to plot coords (x, y)\n",
                "    x, y = j, M - 1 - i\n",
                "    \n",
                "    # Skip barriers and terminals\n",
                "    if gw.is_barrier(state) or gw.is_terminal(state):\n",
                "        # Optionally mark barriers/terminals (e.g., with color)\n",
                "        if gw.is_barrier(state):\n",
                "             ax.add_patch(plt.Rectangle((x-0.5, y-0.5), 1, 1, facecolor='gray'))\n",
                "        elif state == (2, 3): # Positive reward goal state\n",
                "             ax.add_patch(plt.Rectangle((x-0.5, y-0.5), 1, 1, facecolor='lightgreen')) # Goal\n",
                "        elif gw.is_terminal(state): # Other terminal states (e.g., (1, 3))\n",
                "             ax.add_patch(plt.Rectangle((x-0.5, y-0.5), 1, 1, facecolor='salmon'))\n",
                "        continue\n",
                "        \n",
                "    dx, dy = 0, 0\n",
                "    if action == 'up':\n",
                "        dy = 0.5\n",
                "    elif action == 'down':\n",
                "        dy = -0.5\n",
                "    elif action == 'left':\n",
                "        dx = -0.5\n",
                "    elif action == 'right':\n",
                "        dx = 0.5\n",
                "        \n",
                "    # Draw arrow if action is defined\n",
                "    if dx != 0 or dy != 0:\n",
                "        ax.arrow(x - dx*0.3, y - dy*0.3, dx*0.6, dy*0.6, **arrow_props)\n",
                "\n",
                "# Invert y-axis to match grid convention (origin top-left)\n",
                "# ax.invert_yaxis() # Keep origin bottom-left for standard plot view\n",
                "ax.set_aspect('equal', adjustable='box') # Ensure squares are square\n",
                "plt.title('Learned Policy Visualization')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "83be1be1",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

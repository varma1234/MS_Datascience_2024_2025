{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sutton and Barto, Reinforcement Learning 2nd. Edition, page 83.\n",
    "\n",
    "![Sutton and Barto, Reinforcement Learning 2nd. Edition.](./Figures/ValueIteration.png)\n",
    "Value Iteration, for estimating Ï€\n",
    "\n",
    "\n",
    "\n",
    "**Value Iteration**\n",
    "\n",
    "\n",
    "\n",
    "Given a **policy**, one finds the associated **values** using the **iterative policy evaluation** algorithm. Given **values**, one can find the associated **policy**.  Iterating policy evaluation and finding a policy until the policy does not change is the **policy iteration** algorithm.\n",
    "\n",
    "\n",
    "\n",
    "**Value iteration** proceeds by interleaving value calculations with policy updates.  Convergence occurs when the values do not change.\n",
    "\n",
    "\n",
    "\n",
    "Note that for this algorithm one does not need an initial policy.\n",
    "\n",
    "\n",
    "\n",
    "**Value Iteration Algorithm**\n",
    "\n",
    "```python\n",
    "\n",
    "initialize values (to zero, or randomly)\n",
    "\n",
    "while not converged:\n",
    "\n",
    "    for each state \n",
    "\n",
    "        for each decision (at each state)\n",
    "\n",
    "             value = max(reward + gamma*value at dest)\n",
    "\n",
    "compute policy from values\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries\n",
    "\n",
    "Import functions required for creating the grid world environment and computing the policy from state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlgridworld.standard_grid import create_standard_grid\n",
    "from rlgridworld.algorithms import compute_policy_from_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Value Iteration Function\n",
    "\n",
    "Implement the Value Iteration algorithm as described in Sutton and Barto (page 83). This function iteratively updates the value of each state based on the maximum expected return achievable from that state, until the values converge (the maximum change in value across all states is below a small threshold `epsilon`). Unlike Policy Iteration, it doesn't explicitly store or iterate on a policy during the value calculation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from page 83 of Sutton and Barto, RL 2nd. Ed.\n",
    "def value_iteration(gw, gamma=0.9, epsilon=0.001):\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        biggest_change_in_value = 0\n",
    "        for node in gw:\n",
    "            state = node.state\n",
    "            if not gw.is_terminal(state) and not gw.is_barrier(state):\n",
    "                old_value = gw.get_value(state)\n",
    "                new_value = float('-inf')\n",
    "                # valid decisions and rewards at current state\n",
    "                dr = gw.valid_decisions_and_rewards(state)\n",
    "                for action, reward in dr.items():\n",
    "                    reward = gw.get_reward_for_action(state, action)\n",
    "                    value_at_dest = gw.get_value_at_destination(state, action)\n",
    "                    value = reward + gamma*value_at_dest\n",
    "                    if value > new_value:\n",
    "                        new_value = value\n",
    "                    gw.set_value(state, new_value)\n",
    "                biggest_change_in_value = max(biggest_change_in_value,\n",
    "                                                  abs(new_value - old_value))\n",
    "        if biggest_change_in_value < epsilon:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Grid World and Run Value Iteration\n",
    "\n",
    "1.  Instantiate the standard grid world environment.\n",
    "2.  Print the initial state values (usually initialized to zero).\n",
    "3.  Execute the `value_iteration` function to compute the optimal state values. This function modifies the values stored within the `gw` object.\n",
    "4.  Print the converged state values after value iteration.\n",
    "5.  Compute the optimal policy derived from the final state values using `compute_policy_from_values`.\n",
    "6.  Print the resulting optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Values\n",
      "-------------------------------------\n",
      "|   0.00 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.00 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.00 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n",
      "\n",
      "Values after Value Iteration\n",
      "-------------------------------------\n",
      "|   0.81 |   0.90 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.73 |   0.00 |   0.90 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.66 |   0.73 |   0.81 |   0.73 |\n",
      "-------------------------------------\n",
      "\n",
      "Optimal Policy from Value Iteration\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |     Up |        |\n",
      "-------------------------------------\n",
      "|  Right |  Right |     Up |   Left |\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gw = create_standard_grid()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Initial Values\")\n",
    "gw.print_values()\n",
    "\n",
    "# compute values using Value Iteration\n",
    "# This modifies the values within the 'gw' object directly\n",
    "value_iteration(gw)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Values after Value Iteration\")\n",
    "gw.print_values()\n",
    "\n",
    "# compute policy from the final values stored in 'gw'\n",
    "policy = compute_policy_from_values(gw)\n",
    "\n",
    "print(\"\") \n",
    "print(\"Optimal Policy from Value Iteration\")\n",
    "gw.print_policy(policy)\n",
    "print(\"\") # Add a final newline for cleaner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

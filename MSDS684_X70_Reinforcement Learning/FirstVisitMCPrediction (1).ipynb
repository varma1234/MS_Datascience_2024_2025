{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sutton and Barto, Reinforcement Learning 2nd. Edition, page 92.\n",
    "\n",
    "![Sutton and Barto, Reinforcement Learning 2nd. Edition.](./Figures/FirstVisitMCPrediction.png)\n",
    "First-visit MC prediction, for estimating V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup\n",
    "\n",
    "This cell imports necessary libraries (`numpy` for numerical operations and `create_standard_grid` from a custom module `rlgridworld`) and sets a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlgridworld.standard_grid import create_standard_grid\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `play_game` Function\n",
    "\n",
    "This function simulates playing one episode of the grid world game according to a given policy. \n",
    "\n",
    "- It starts from a specified state (defaulting to (0,0)).\n",
    "- In a loop, it determines the action based on the policy for the current state.\n",
    "- It gets the reward for taking that action in the current state.\n",
    "- It determines the next state using the `move` function.\n",
    "- It records the (state, reward) pair.\n",
    "- The loop continues until a terminal state is reached.\n",
    "- Finally, it returns a list of (state, reward) tuples encountered during the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(gw, policy, state=(0,0)):\n",
    "    # default game starting point is state = (0,0) \n",
    "    # list of tuples that are (state, reward) pairs\n",
    "    states_and_rewards = [] \n",
    "    converged = False\n",
    "    while not converged:\n",
    "        # get action from policy\n",
    "        action = policy[state] \n",
    "        # find reward for the action\n",
    "        reward = gw.get_reward_for_action(state, action)\n",
    "        # move to the new state\n",
    "        stateprime = move(state,action)\n",
    "        # add new state and reward to the list\n",
    "        states_and_rewards.append((state,reward))\n",
    "        # if you have moved to a terminal state, then stop\n",
    "        if gw.is_terminal(stateprime):\n",
    "            converged = True\n",
    "        # update state to new state\n",
    "        state = stateprime\n",
    "    return states_and_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `move` Function\n",
    "\n",
    "This helper function calculates the next state based on the current state (i, j coordinates) and the chosen action ('up', 'down', 'left', 'right'). It assumes the action provided is valid for the state (i.e., doesn't lead into a wall immediately, although the `play_game` function handles terminal states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(state, action): # only valid actions at states are sent to move\n",
    "    i,j = state\n",
    "    if action == 'left':\n",
    "        j = j-1\n",
    "    if action == 'right':\n",
    "        j = j+1\n",
    "    if action == 'down':\n",
    "        i = i-1\n",
    "    if action == 'up':\n",
    "        i = i+1\n",
    "    return (i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `compute_G` Function\n",
    "\n",
    "This function calculates the discounted return Gt for each state visited during an episode.\n",
    "\n",
    "- It takes the list of (state, reward) pairs from `play_game` and a discount factor `gamma`.\n",
    "- It iterates through the episode's steps *in reverse*.\n",
    "- For each step (s, r), it calculates the return G using the formula: G = r + gamma * G (where G is the return calculated from the subsequent steps).\n",
    "- It stores the (state, G) pair.\n",
    "- Finally, it reverses the list back to the original time order and returns the list of (state, return) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_G(states_and_rewards, gamma):\n",
    "    # function computes G(s) for states visited on trajectory of visited states during play_game()\n",
    "    # input is states_and_rewards list returned from play_game\n",
    "    # Note similarity in names: states_and_rewards is generated by play_game(). This function \n",
    "    # generates states_and_returns, which is a list of states and the associated G(s) value for the\n",
    "    # state. \n",
    "    G = 0\n",
    "    states_and_returns = [] # list of tuples that are (state, return) pairs\n",
    "    for s, r in reversed(states_and_rewards):\n",
    "        G = r + gamma*G\n",
    "        states_and_returns.append((s,G))\n",
    "    states_and_returns.reverse()\n",
    "    return(states_and_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Grid World Environment\n",
    "\n",
    "This cell initializes the standard grid world environment using the imported `create_standard_grid` function. The `gw` object represents the environment, holding information about states, actions, rewards, and transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = create_standard_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Policy\n",
    "\n",
    "This cell defines the policy (Ï€) that we want to evaluate. It's represented as a dictionary where keys are state tuples (row, col) and values are the actions ('up', 'down', 'left', 'right', or '' for terminal/barrier states) to take in those states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = { \n",
    "    (0,0):'up', (0,1):'right',(0,2):'right',(0,3):'up',\n",
    "    (1,0):'up', (1,1):'', (1,2):'right', (1,3):'',\n",
    "    (2,0):'right', (2,1):'right', (2,2):'right', (2,3):''\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization for Value Estimation\n",
    "\n",
    "This cell sets up variables needed for the First-Visit Monte Carlo prediction algorithm:\n",
    "- `gamma`: The discount factor (0.9) used in calculating returns.\n",
    "- `all_states`: A list explicitly defining all possible states in the grid (excluding terminal/barrier states implicitly handled later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "all_states = [\n",
    "            (0,0), (0,1) ,(0,2), (0,3),\n",
    "            (1,0), (1,1), (1,2), (1,3),\n",
    "            (2,0), (2,1), (2,2), (2,3)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Single Episode Evaluation\n",
    "\n",
    "This section demonstrates the core steps of First-Visit MC prediction using a single episode generated by following the defined policy from the default start state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play One Episode\n",
    "\n",
    "Calls the `play_game` function with the defined grid world (`gw`) and policy (`policy`) to simulate one complete episode, starting from the default state (0,0). The resulting sequence of (state, reward) pairs is stored in `states_and_rewards`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_and_rewards = play_game(gw, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Episode Trajectory\n",
    "\n",
    "Displays the `states_and_rewards` list generated in the previous step, showing the sequence of states visited and the immediate rewards received at each step during the single episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0), 0.0), ((1, 0), 0.0), ((2, 0), 0.0), ((2, 1), 0.0), ((2, 2), 1.0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_and_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Returns (G) for the Episode\n",
    "\n",
    "Calls the `compute_G` function to calculate the discounted return Gt for each state visited in the episode, using the `states_and_rewards` list and the discount factor `gamma`. The result is stored in `states_and_returns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_and_returns = compute_G(states_and_rewards, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine State Returns\n",
    "\n",
    "Displays the `states_and_returns` list, showing each state visited during the episode paired with its calculated discounted return Gt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0), 0.6561000000000001),\n",
       " ((1, 0), 0.7290000000000001),\n",
       " ((2, 0), 0.81),\n",
       " ((2, 1), 0.9),\n",
       " ((2, 2), 1.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_and_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update State Values (First-Visit MC)\n",
    "\n",
    "This cell implements the core logic of First-Visit Monte Carlo prediction for the single episode:\n",
    "1. Initializes a dictionary `returns` to store the G values encountered for each state across episodes (though here, only one episode is processed).\n",
    "2. Initializes an empty set `seen_states` to track states already visited *within this episode*.\n",
    "3. Iterates through the `states_and_returns` list.\n",
    "4. For each (state, G) pair, if the state has *not* been seen yet in this episode:\n",
    "   - Appends the calculated return `G` to the list associated with that state in the `returns` dictionary.\n",
    "   - Updates the value V(s) for that state in the grid world (`gw`) by taking the mean of all returns collected for that state so far (in this case, just the single G value).\n",
    "   - Adds the state to the `seen_states` set to ensure only the *first* visit's return is used for this episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = {} # add code to not include terminal and barrier states to returns dictionary\n",
    "for s in all_states:\n",
    "    returns[s] = []\n",
    "    \n",
    "seen_states = set()\n",
    "for s, G in states_and_returns:\n",
    "    # print(s,G) # Uncomment to see step-by-step processing\n",
    "    if s not in seen_states:\n",
    "        returns[s].append(G)\n",
    "        gw.set_value(s, np.mean(returns[s])) # mean is for stochastic (or averaging over episodes)\n",
    "        seen_states.add(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show States Seen in Episode\n",
    "\n",
    "Displays the `seen_states` set, showing which unique states were visited at least once during the single episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0), (1, 0), (2, 0), (2, 1), (2, 2)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Updated Grid Values\n",
    "\n",
    "Calls the `print_values` method on the grid world object (`gw`) to display the current estimated state values (V(s)). After processing only one episode, these values are based solely on the returns from that single trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "|   0.81 |   0.90 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.73 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.66 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gw.print_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Collected Returns\n",
    "\n",
    "Displays the `returns` dictionary, showing the list of returns collected for each state. After one episode, each visited state will have a list containing just one return value (the Gt from its first visit in that episode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): [0.6561000000000001],\n",
       " (0, 1): [],\n",
       " (0, 2): [],\n",
       " (0, 3): [],\n",
       " (1, 0): [0.7290000000000001],\n",
       " (1, 1): [],\n",
       " (1, 2): [],\n",
       " (1, 3): [],\n",
       " (2, 0): [0.81],\n",
       " (2, 1): [0.9],\n",
       " (2, 2): [1.0],\n",
       " (2, 3): []}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Multiple Episodes (Fixed Start)\n",
    "\n",
    "This section repeats the process from Section 1 multiple times (10 episodes) but always starts from the same initial state (0,0). This highlights a potential issue with MC methods if exploration is insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 10 Episodes (Fixed Start)\n",
    "\n",
    "This cell runs the First-Visit MC prediction process for `num_episodes` (10) times:\n",
    "1. **Loop:** Iterates 10 times.\n",
    "2. **Play Episode:** Calls `play_game` starting from the default state (0,0).\n",
    "3. **Compute Returns:** Calculates `states_and_returns` for the episode using `compute_G`.\n",
    "4. **Initialize `returns`:** Resets the `returns` dictionary for *each episode* (Note: This is incorrect for accumulating averages across episodes. The `returns` dictionary should persist and accumulate outside the loop for proper MC averaging. The current code effectively only uses the *last* episode's returns to set the values).\n",
    "5. **First-Visit Update:** Iterates through the episode's `states_and_returns`, updating `returns` and `gw` values only for the first visit to each state *within that specific episode*.\n",
    "\n",
    "**Important Note:** Because `returns` is re-initialized inside the loop and `gw.set_value` overwrites the value based only on the current episode's first-visit return, this code doesn't correctly average returns across episodes as intended by the standard First-Visit MC algorithm. It essentially re-evaluates based on each new episode independently, overwriting previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "for t in range(num_episodes):\n",
    "    # Play episode from default start\n",
    "    states_and_rewards = play_game(gw, policy)\n",
    "    \n",
    "    # Compute returns for the episode\n",
    "    G = 0\n",
    "    states_and_returns = []\n",
    "    for s, r in reversed(states_and_rewards):\n",
    "        G = r + gamma*G\n",
    "        states_and_returns.append((s,G))\n",
    "    states_and_returns.reverse()\n",
    "    \n",
    "    # *** Issue: returns should accumulate across episodes, not reset ***\n",
    "    returns = {} \n",
    "    for s in all_states:\n",
    "        returns[s] = []\n",
    "    \n",
    "    # First-visit update for the current episode\n",
    "    seen_states = set()\n",
    "    for s, G in states_and_returns:\n",
    "        if s not in seen_states:\n",
    "            returns[s].append(G) # Appends to the per-episode returns dict\n",
    "            # Overwrites V(s) based only on this episode's first visit G\n",
    "            gw.set_value(s, np.mean(returns[s])) \n",
    "            seen_states.add(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Grid Values After 10 Fixed-Start Episodes\n",
    "\n",
    "Prints the state values V(s) in the grid world after running the loop 10 times. Because the environment and policy are deterministic and the starting state is fixed, each episode follows the exact same path. Due to the implementation issue noted above (resetting `returns` and overwriting V(s) each time), the final values simply reflect the result from the 10th (last) identical episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "|   0.81 |   0.90 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.73 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.66 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gw.print_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Fixed-Start Results\n",
    "\n",
    "This markdown cell correctly observes that since the grid world and policy are deterministic, starting from the same state always produces the same trajectory. Therefore, running 10 episodes yields the same result each time. It also notes that states in the bottom row are never visited because the policy immediately directs the agent upwards from the start state (0,0) and subsequent states visited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Multiple Episodes (Random Starts)\n",
    "\n",
    "This section addresses the exploration problem highlighted in Section 2 by using *exploring starts*. Each episode begins from a randomly chosen non-terminal, non-barrier state. This ensures that, over many episodes, all states have a chance of being visited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 100 Episodes (Random Starts)\n",
    "\n",
    "This cell runs the First-Visit MC prediction process for `num_episodes` (100) times, incorporating exploring starts:\n",
    "1. **Loop:** Iterates 100 times.\n",
    "2. **Select Random Start:**\n",
    "   - Picks a random index within the `all_states` list.\n",
    "   - Gets the corresponding state.\n",
    "   - **Validation:** Ensures the chosen state is not a barrier or terminal state; if it is, it re-picks until a valid start state is found.\n",
    "3. **Play Episode:** Calls `play_game` starting from the randomly selected `state`.\n",
    "4. **Compute Returns:** Calculates `states_and_returns` for the episode.\n",
    "5. **Initialize `returns`:** Resets the `returns` dictionary for *each episode* (Same issue as Section 2: this prevents proper averaging across episodes).\n",
    "6. **First-Visit Update:** Updates `returns` and `gw` values based only on the first visit to each state *within the current episode*.\n",
    "\n",
    "**Note:** While exploring starts helps visit more states, the implementation flaw of resetting `returns` and overwriting V(s) each episode persists. The final V(s) will be heavily influenced by the *last* episode that happened to visit state s for the first time in that episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "for t in range(num_episodes):\n",
    "    # select a random starting point in the list of all states\n",
    "    random_indx = np.random.randint(0,len(all_states)) \n",
    "    # get the state \n",
    "    state = all_states[random_indx]\n",
    "    # if the selected random state is a barrier state or a terminal state, then do it again\n",
    "    while(gw.is_barrier(state) or gw.is_terminal(state)):\n",
    "        random_indx = np.random.randint(0,len(all_states))\n",
    "        state = all_states[random_indx]  \n",
    "        \n",
    "    # play the game from the selected state\n",
    "    states_and_rewards = play_game(gw, policy, state=state)\n",
    "    \n",
    "    # compute states_and_returns\n",
    "    G = 0\n",
    "    states_and_returns = []\n",
    "    for s, r in reversed(states_and_rewards):\n",
    "        G = r + gamma*G\n",
    "        states_and_returns.append((s,G))\n",
    "    states_and_returns.reverse()\n",
    "    \n",
    "    # *** Issue: returns should accumulate across episodes, not reset ***\n",
    "    returns = {} \n",
    "    for s in all_states:\n",
    "        returns[s] = []\n",
    "    \n",
    "    # perform rest of calculation on the \n",
    "    seen_states = set()\n",
    "    for s, G in states_and_returns:\n",
    "        if s not in seen_states:\n",
    "            returns[s].append(G)\n",
    "            # Overwrites V(s) based only on this episode's first visit G\n",
    "            gw.set_value(s, np.mean(returns[s])) \n",
    "            seen_states.add(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Grid Values After 100 Random-Start Episodes\n",
    "\n",
    "Prints the state values V(s) after 100 episodes with exploring starts. Because random starting points allow different trajectories to be explored, states that were previously unvisited (like those in the bottom row) now have estimated values. However, due to the implementation issue, these values are not the result of averaging returns across all 100 episodes but are influenced primarily by the last episode that visited each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "|   0.81 |   0.90 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.73 |   0.00 |  -1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.66 |  -0.81 |  -0.90 |  -1.00 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gw.print_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Policy\n",
    "\n",
    "Calls the `print_policy` method on the grid world object (`gw`) to display the input policy that was being evaluated throughout the notebook. This helps visualize the actions taken in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |  Right |  Right |     Up |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gw.print_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Observation\n",
    "\n",
    "This markdown cell notes that the values obtained using exploring starts appear more reasonable, particularly for the previously unvisited lower row states, compared to the fixed-start approach. It implies that the values now better reflect the expected returns under the given policy when starting from various states."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

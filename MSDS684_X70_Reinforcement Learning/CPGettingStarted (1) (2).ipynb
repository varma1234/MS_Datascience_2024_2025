{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf68b55",
   "metadata": {},
   "source": [
    "### Getting Started with Cart Pole in OpenAI Gym\n",
    "\n",
    "This notebook provides a basic introduction to interacting with the classic 'CartPole-v1' environment using the Gymnasium library (the successor to OpenAI Gym). The goal in the CartPole environment is to balance a pole upright on a movable cart by applying forces (left or right) to the cart. An episode ends if the pole tilts too far, the cart moves off the track, or a maximum number of steps is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09cf13",
   "metadata": {},
   "source": [
    "#### October 25, 2022 update\n",
    "\n",
    "- [Announcing The Farama Foundation](https://farama.org/Announcing-The-Farama-Foundation) The future of open source reinforcement learning\n",
    "\n",
    "- The Farama Foundation is a non profit designed to house reinforcement learning libraries in a neutral nonprofit body.\n",
    "\n",
    "- Released the [Gymnasium](https://github.com/Farama-Foundation/Gymnasium) library. Future maintenance of OpenAI Gym will take place here.\n",
    "\n",
    "- Gym documentation is at [https://www.gymlibrary.dev/index.html](https://www.gymlibrary.dev/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac5778",
   "metadata": {},
   "source": [
    "#### Installing gymnasium\n",
    "\n",
    "If you haven't installed Gymnasium yet, you can do so from the command prompt (or a terminal within your environment) using pip:\n",
    "\n",
    "``pip install gymnasium``\n",
    "\n",
    "You might also need specific extras for rendering or different environments, e.g., `pip install gymnasium[classic_control]` for the CartPole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4bd73",
   "metadata": {},
   "source": [
    "#### Import Gymnasium Library\n",
    "\n",
    "First, we import the necessary library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f46e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Gymnasium library\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81215582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gymnasium version: 1.1.1\n"
     ]
    }
   ],
   "source": [
    "# Optional: Check the installed version of Gymnasium\n",
    "print(f\"Gymnasium version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16456c",
   "metadata": {},
   "source": [
    "#### Create the Cart Pole environment\n",
    "\n",
    "The `gym.make()` function is used to instantiate an environment. We specify the environment ID, in this case, `'CartPole-v1'`. \n",
    "\n",
    "Once created, we can inspect its `observation_space` and `action_space` properties to understand the format of the states we'll receive and the actions we can take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bee5b",
   "metadata": {},
   "source": [
    "#### Create Cart Pole Environment and Inspect Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f1a16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action Space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Print information about the observation and action spaces\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Action Space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620093e",
   "metadata": {},
   "source": [
    "The `observation_space` is a `Box(4,)`, meaning it's a 4-dimensional continuous space. The values represent:\n",
    "\n",
    "1.  **Cart Position:** How far the cart is from the center.\n",
    "2.  **Cart Velocity:** How fast the cart is moving.\n",
    "3.  **Pole Angle:** The angle of the pole with respect to the vertical position (0 is upright).\n",
    "4.  **Pole Angular Velocity:** How fast the pole is rotating.\n",
    "\n",
    "The bounds shown (`low` and `high`) indicate the theoretical limits, although episodes often terminate before these are reached (e.g., pole angle > 12 degrees or cart position > 2.4 units)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659eae9",
   "metadata": {},
   "source": [
    "The `action_space` is `Discrete(2)`, meaning there are two possible discrete actions:\n",
    "\n",
    "- **0:** Push the cart to the left.\n",
    "- **1:** Push the cart to the right.\n",
    "\n",
    "The agent needs to learn a policy (a way to choose actions based on observations) to keep the pole balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3153df6",
   "metadata": {},
   "source": [
    "#### Reset (start) the environment and get an Initial Observation\n",
    "\n",
    "Before starting an interaction loop (an episode), we must `reset()` the environment. This function:\n",
    "- Puts the environment into a valid starting state (e.g., cart near the center, pole nearly upright).\n",
    "- Returns the initial `observation` corresponding to this starting state.\n",
    "- Returns an `info` dictionary which might contain auxiliary information (often empty for simple environments at reset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a7842d",
   "metadata": {},
   "source": [
    "#### Reset Environment for New Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f0a9da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: [ 0.00447542 -0.02835283  0.04589985 -0.00298108]\n",
      "Initial Env info: {}\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment to start a new episode\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(f\"Initial Observation: {obs}\")\n",
    "print(f\"Initial Env info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe1ecc",
   "metadata": {},
   "source": [
    "The observation array returned contains the initial values for the cart position, cart velocity, pole angle, and pole angular velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4019c93",
   "metadata": {},
   "source": [
    "#### Actions, Rewards, and a New Observation\n",
    "\n",
    "The core interaction loop involves taking an `action` in the environment using the `env.step(action)` method. This function simulates one time step in the environment based on the chosen action.\n",
    "\n",
    "It returns five values:\n",
    "- `observation`: The next state of the environment after taking the action.\n",
    "- `reward`: A scalar value indicating the immediate reward received for the action taken in the previous state. In CartPole, this is typically +1 for every step the pole remains balanced.\n",
    "- `terminated`: A boolean flag. `True` if the episode ended due to reaching a terminal state (e.g., pole fell over, cart went off-screen).\n",
    "- `truncated`: A boolean flag. `True` if the episode ended due to an external condition (e.g., reaching a time limit, like 500 steps in CartPole-v1).\n",
    "- `info`: A dictionary containing auxiliary diagnostic information (often empty)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a1a90",
   "metadata": {},
   "source": [
    "#### Take a Step in the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b688cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Taken: 0\n",
      "New Observation: [ 0.00390836 -0.22410198  0.04584023  0.3038232 ]\n",
      "Reward received: 1.0\n",
      "Episode terminated: False\n",
      "Episode truncated: False\n",
      "Step Info dictionary: {}\n"
     ]
    }
   ],
   "source": [
    "# Perform an action (e.g., push left)\n",
    "action = 0 \n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(f\"Action Taken: {action}\")\n",
    "print(f\"New Observation: {obs}\")\n",
    "print(f\"Reward received: {reward}\")\n",
    "print(f\"Episode terminated: {terminated}\")\n",
    "print(f\"Episode truncated: {truncated}\")\n",
    "print(f\"Step Info dictionary: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc361c",
   "metadata": {},
   "source": [
    "#### Running a Simple Loop\n",
    "\n",
    "Let's run a short loop taking random actions to see how the episode progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c897c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running a short episode with random actions ---\n",
      "\n",
      "Step: 1\n",
      "Taking action: 0\n",
      "Observation: [ 0.04135794 -0.20590533  0.01851772  0.3429478 ]\n",
      "Reward: 1.0\n",
      "Terminated: False, Truncated: False\n",
      "\n",
      "Step: 2\n",
      "Taking action: 1\n",
      "Observation: [ 0.03723983 -0.01105165  0.02537668  0.0561613 ]\n",
      "Reward: 1.0\n",
      "Terminated: False, Truncated: False\n",
      "\n",
      "Step: 3\n",
      "Taking action: 0\n",
      "Observation: [ 0.0370188  -0.2065281   0.0264999   0.35674152]\n",
      "Reward: 1.0\n",
      "Terminated: False, Truncated: False\n",
      "\n",
      "Step: 4\n",
      "Taking action: 0\n",
      "Observation: [ 0.03288824 -0.40201658  0.03363473  0.6576613 ]\n",
      "Reward: 1.0\n",
      "Terminated: False, Truncated: False\n",
      "\n",
      "Step: 5\n",
      "Taking action: 1\n",
      "Observation: [ 0.0248479  -0.20737855  0.04678796  0.37575617]\n",
      "Reward: 1.0\n",
      "Terminated: False, Truncated: False\n",
      "\n",
      "Step: 6\n",
      "Taking action: 0\n",
      "Observation: [ 0.02070033 -0.40313277  0.05430308  0.6828168 ]\n",
      "Reward: 1.0\n",
      "Terminated: False, Truncated: False\n",
      "\n",
      "Step: 7\n",
      "Taking action: 0\n",
      "Observation: [ 0.01263768 -0.59896505  0.06795942  0.9920895 ]\n",
      "Reward: 1.0\n",
      "Terminated: False, Truncated: False\n",
      "\n",
      "Step: 8\n",
      "Taking action: 0\n",
      "Observation: [ 6.5837684e-04 -7.9492736e-01  8.7801211e-02  1.3053191e+00]\n",
      "Reward: 1.0\n",
      "Terminated: False, Truncated: False\n",
      "\n",
      "Step: 9\n",
      "Taking action: 0\n",
      "Observation: [-0.01524017 -0.9910459   0.11390759  1.6241442 ]\n",
      "Reward: 1.0\n",
      "Terminated: False, Truncated: False\n",
      "\n",
      "Step: 10\n",
      "Taking action: 1\n",
      "Observation: [-0.03506109 -0.797434    0.14639047  1.3690248 ]\n",
      "Reward: 1.0\n",
      "Terminated: False, Truncated: False\n",
      "\n",
      "Total reward for this short random episode: 10.0\n"
     ]
    }
   ],
   "source": [
    "# Example: Take a few random steps\n",
    "print(\"\\n--- Running a short episode with random actions ---\")\n",
    "obs, info = env.reset() # Start a new episode\n",
    "total_reward = 0\n",
    "for step_num in range(10): # Run for a maximum of 10 steps\n",
    "    random_action = env.action_space.sample() # Sample a random action (0 or 1)\n",
    "    print(f\"\\nStep: {step_num + 1}\")\n",
    "    print(f\"Taking action: {random_action}\")\n",
    "    obs, reward, terminated, truncated, info = env.step(random_action)\n",
    "    total_reward += reward\n",
    "    print(f\"Observation: {obs}\")\n",
    "    print(f\"Reward: {reward}\")\n",
    "    print(f\"Terminated: {terminated}, Truncated: {truncated}\")\n",
    "    \n",
    "    # Check if the episode has ended\n",
    "    if terminated or truncated:\n",
    "        print(f\"\\nEpisode finished after {step_num + 1} steps.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal reward for this short random episode: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd0617",
   "metadata": {},
   "source": [
    "#### Closing the Environment\n",
    "\n",
    "It's good practice to close the environment when you're done with it, especially if it involves graphical rendering or other resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ac21d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fade20",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "[^1]: In earlier versions of OpenAI gym the ``env.reset()`` function had a different signature and return values. The current Gymnasium API returns `observation, info`.\n",
    "\n",
    "[^2]: Similarly, the ``env.step(action)`` function in earlier versions returned `observation, reward, done, info`. The current Gymnasium API returns `observation, reward, terminated, truncated, info`, splitting the old `done` flag into `terminated` (environment-specific end) and `truncated` (external condition like time limit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b0444",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

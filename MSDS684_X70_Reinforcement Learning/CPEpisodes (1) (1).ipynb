{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to CartPole Episodes\n",
    "\n",
    "This markdown cell provides a high-level overview of what constitutes an \"episode\" in the context of the CartPole environment within OpenAI Gym (now Gymnasium). It explains the key concepts: reset, observation, action, step, reward, policy, and value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodes of the Cart Pole Game\n",
    "\n",
    "\n",
    "\n",
    "An episode of begins by calling the ``reset`` function. This returns the **observation** values associated with the initial state of the MDP.\n",
    "\n",
    "\n",
    "\n",
    "At each state, an **action** must be chosen that is then sent to the ``step`` function. The step function returns a **new observation** and the **reward** that resulted from that action.\n",
    "\n",
    "\n",
    "\n",
    "The process of selecting an action that results a new observation and a reward is repeated until the MDP terminates. The step function also returns a flag that indicates whether it has terminated, or not.\n",
    "\n",
    "\n",
    "\n",
    "An **episode** of the CartPole MDP starts by calling the reset function and then repeatedly calling the step function until the MDP terminates. The set of actions taken is called the **policy** and the sum of the rewards received is called the **value** of the policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Creating the Environment\n",
    "\n",
    "This code cell imports the necessary library (`gymnasium`) and creates an instance of the CartPole environment (`CartPole-v1`). This environment object (`env`) will be used to interact with the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import OpenAI Gym\n",
    "import gymnasium as gym\n",
    "#create CartPole environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting an Episode and Observing the Initial State\n",
    "\n",
    "This markdown cell explains the purpose of the `reset()` function, which initializes the environment to a starting state and returns the first observation. It also notes the inherent randomness in the starting state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Starting Cart Pole\n",
    "\n",
    "- Start and episode of Cart Pole using the reset() function  \n",
    "\n",
    "- Print the observed values - Note these values have a random component to them and will not be the same every time you reset the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resetting the Environment and Printing the Initial Observation\n",
    "\n",
    "This code cell demonstrates how to start (or reset) an episode using `env.reset()`. It captures the initial observation and prints it. The `_` is used to ignore the secondary return value (info dictionary) from `reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [-0.04027831  0.01710224 -0.03783857 -0.03566743]\n"
     ]
    }
   ],
   "source": [
    "#start an episode \n",
    "obs,_ = env.reset()\n",
    "print(f\"Observation: {obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Actions in CartPole\n",
    "\n",
    "This markdown cell describes how actions are selected, specifically mentioning the `sample()` method for choosing a random action from the available action space (0 or 1 for CartPole)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting actions\n",
    "\n",
    "- The sample() function returns a random action from the action space  \n",
    "\n",
    "- For Cart Pole, the actions are 0 and 1\n",
    "\n",
    "\n",
    "\n",
    "The code in the next cell print a small set of actions from the sample function to see the different values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Random Actions\n",
    "\n",
    "This code cell demonstrates the use of `env.action_space.sample()` to get random actions. It runs a loop to show that the sampled actions are indeed random (either 0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 0: 0\n",
      "Action 1: 1\n",
      "Action 2: 0\n",
      "Action 3: 1\n",
      "Action 4: 1\n",
      "Action 5: 0\n",
      "Action 6: 0\n",
      "Action 7: 1\n"
     ]
    }
   ],
   "source": [
    "# sample() returns a random action\n",
    "for i in range(8):\n",
    "    action = env.action_space.sample()\n",
    "    print(f\"Action {i}: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Full Episode with Random Actions\n",
    "\n",
    "This markdown cell outlines the process of running a complete episode. It involves resetting the environment and then looping through steps (taking random actions) until the episode terminates (`terminated` flag becomes True). It details what information is obtained and printed at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### An episode of Cart Pole\n",
    "\n",
    "An episode of Cart Pole starts using the reset function. Then while the done boolean variable is false, an action is selected and passed to the step function that executes the step. The step function returns a new observation, a reward, and whether the episode has terminated, or not, with the done boolean variable.  \n",
    "\n",
    "- Run an episode of Cart Pole by taking a random action at each step of the episode  \n",
    "\n",
    "- Reset to start an episode\n",
    "\n",
    "- While not done\n",
    "\n",
    "    - Get a random action\n",
    "\n",
    "    - Perform the action and record results\n",
    "\n",
    "    - Print \n",
    "\n",
    "        - step number \n",
    "\n",
    "        - action taken \n",
    "\n",
    "        - new observation \n",
    "\n",
    "        - reward\n",
    "\n",
    "        - terminated flag\n",
    "\n",
    "        - truncated flag  \n",
    "\n",
    "- The episode terminates when the terminated flag is True\n",
    "\n",
    "\n",
    "\n",
    "The cell below runs an episode of the Cart Pole MDP. Actions are selected at random. Run the cell a number of times to observe that the number of steps in the MDP varies. Note in the final step that ther **terminated** flag has been set to True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Running a Single Episode\n",
    "\n",
    "This code cell implements the logic described above. It resets the environment, then enters a `while` loop that continues as long as the episode is not `terminated`. Inside the loop, it samples a random action, takes a step in the environment using `env.step(action)`, updates the total reward, and prints the details of the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: [ 0.02208824  0.00818464 -0.02375799  0.03528415]\n",
      "Step: 1, Action: 0, New Observtion: [ 0.02225194 -0.1865887  -0.02305231  0.32037753], Reward: 1.0\n",
      "   --- Sum of Rewards: 1.0, Terminated: False, Truncated: False \n",
      "Step: 2, Action: 0, New Observtion: [ 0.01852016 -0.3813749  -0.01664476  0.6057024 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 2.0, Terminated: False, Truncated: False \n",
      "Step: 3, Action: 0, New Observtion: [ 0.01089266 -0.5762602  -0.00453071  0.8930965 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 3.0, Terminated: False, Truncated: False \n",
      "Step: 4, Action: 1, New Observtion: [-0.00063254 -0.38107708  0.01333122  0.5989928 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 4.0, Terminated: False, Truncated: False \n",
      "Step: 5, Action: 0, New Observtion: [-0.00825408 -0.576383    0.02531108  0.89584494], Reward: 1.0\n",
      "   --- Sum of Rewards: 5.0, Terminated: False, Truncated: False \n",
      "Step: 6, Action: 0, New Observtion: [-0.01978174 -0.7718388   0.04322797  1.1963754 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 6.0, Terminated: False, Truncated: False \n",
      "Step: 7, Action: 1, New Observtion: [-0.03521852 -0.5773023   0.06715549  0.9175483 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 7.0, Terminated: False, Truncated: False \n",
      "Step: 8, Action: 1, New Observtion: [-0.04676456 -0.3831494   0.08550645  0.6467043 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 8.0, Terminated: False, Truncated: False \n",
      "Step: 9, Action: 1, New Observtion: [-0.05442755 -0.18931636  0.09844054  0.38212526], Reward: 1.0\n",
      "   --- Sum of Rewards: 9.0, Terminated: False, Truncated: False \n",
      "Step: 10, Action: 0, New Observtion: [-0.05821388 -0.3856882   0.10608304  0.70415175], Reward: 1.0\n",
      "   --- Sum of Rewards: 10.0, Terminated: False, Truncated: False \n",
      "Step: 11, Action: 0, New Observtion: [-0.06592764 -0.5821078   0.12016608  1.0282549 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 11.0, Terminated: False, Truncated: False \n",
      "Step: 12, Action: 1, New Observtion: [-0.0775698  -0.3887723   0.14073117  0.7755867 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 12.0, Terminated: False, Truncated: False \n",
      "Step: 13, Action: 1, New Observtion: [-0.08534525 -0.19583753  0.1562429   0.5302842 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 13.0, Terminated: False, Truncated: False \n",
      "Step: 14, Action: 0, New Observtion: [-0.08926199 -0.39277235  0.16684859  0.86783683], Reward: 1.0\n",
      "   --- Sum of Rewards: 14.0, Terminated: False, Truncated: False \n",
      "Step: 15, Action: 0, New Observtion: [-0.09711744 -0.58972347  0.18420532  1.2079872 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 15.0, Terminated: False, Truncated: False \n",
      "Step: 16, Action: 0, New Observtion: [-0.10891191 -0.78668356  0.20836507  1.5522797 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 16.0, Terminated: False, Truncated: False \n",
      "Step: 17, Action: 0, New Observtion: [-0.12464558 -0.9836052   0.23941067  1.9020902 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 17.0, Terminated: True, Truncated: False \n"
     ]
    }
   ],
   "source": [
    "obs,_ = env.reset()\n",
    "print(f\"Initial Observation: {obs}\")\n",
    "i = 0 # counts the number of steps in the episode\n",
    "sum_reward = 0.0 # sums the rewards\n",
    "terminated = False\n",
    "truncated = False # Initialize truncated flag\n",
    "\n",
    "while not terminated and not truncated: # Check for truncated as well\n",
    "    action = env.action_space.sample()\n",
    "    obs,reward,terminated,truncated,_ = env.step(action)\n",
    "    i += 1\n",
    "    sum_reward += reward\n",
    "    print(f\"Step: {i}, Action: {action}, New Observtion: {obs}, Reward: {reward}\")\n",
    "    print(f\"   --- Sum of Rewards: {sum_reward}, Terminated: {terminated}, Truncated: {truncated} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavior After Episode Termination\n",
    "\n",
    "This markdown cell explains what happens if you try to take another step *after* an episode has already terminated. It highlights that the `terminated` flag remains `True` and no further reward is accumulated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run another step of the MDP from the terminal state you will get a warning that the behavior of the step function is undefined. However, note that \n",
    "\n",
    "- **terminated** remains True\n",
    "\n",
    "- the **reward** is zero, and consequently\n",
    "\n",
    "- the **sum of the rewards** does not increase "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating a Step After Termination\n",
    "\n",
    "This code cell explicitly takes one more step after the previous episode likely terminated. It shows the output, confirming the points made in the preceding markdown cell (terminated=True, reward=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 18, Action: 1, New Observtion: [-0.14431769 -0.79173946  0.27745247  1.6922164 ], Reward: 0.0\n",
      "   --- Sum of Rewards: 17.0, Terminated: True, Truncated: False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:214: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "obs,reward,terminated,truncated,_ = env.step(action)\n",
    "i += 1\n",
    "sum_reward += reward\n",
    "print(f\"Step: {i}, Action: {action}, New Observtion: {obs}, Reward: {reward}\")\n",
    "print(f\"   --- Sum of Rewards: {sum_reward}, Terminated: {terminated}, Truncated: {truncated} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the Example: Running Multiple Episodes\n",
    "\n",
    "This markdown cell introduces the next code block, which extends the single-episode example. The goal is to run multiple episodes using the random action policy and calculate the average number of steps (episode length) achieved over these runs. This gives a basic measure of the performance of the random policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Multiple Episodes and Averaging Length\n",
    "\n",
    "This code cell runs a specified number of episodes (`num_episodes`). For each episode, it follows the same logic as the single-episode example (reset, loop steps with random actions until termination or truncation). It records the length (number of steps) of each episode. After all episodes are run, it calculates and prints the average episode length. Finally, it closes the environment using `env.close()` to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running 10 episodes with random actions...\n",
      "Episode 1 finished after 28 steps.\n",
      "Episode 2 finished after 21 steps.\n",
      "Episode 3 finished after 23 steps.\n",
      "Episode 4 finished after 26 steps.\n",
      "Episode 5 finished after 26 steps.\n",
      "Episode 6 finished after 27 steps.\n",
      "Episode 7 finished after 15 steps.\n",
      "Episode 8 finished after 16 steps.\n",
      "Episode 9 finished after 73 steps.\n",
      "Episode 10 finished after 12 steps.\n",
      "\n",
      "Average episode length over 10 episodes: 26.70\n"
     ]
    }
   ],
   "source": [
    "# Run multiple episodes and calculate average length\n",
    "num_episodes = 10\n",
    "episode_lengths = []\n",
    "\n",
    "print(f\"\\nRunning {num_episodes} episodes with random actions...\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    steps = 0\n",
    "    # It's good practice to also check for truncation, \n",
    "    # which signifies reaching a time limit even if not 'failed'.\n",
    "    while not terminated and not truncated: \n",
    "        action = env.action_space.sample() # Random action\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        steps += 1\n",
    "    episode_lengths.append(steps)\n",
    "    print(f\"Episode {episode + 1} finished after {steps} steps.\")\n",
    "\n",
    "if episode_lengths: # Avoid division by zero if num_episodes was 0\n",
    "    average_length = sum(episode_lengths) / num_episodes\n",
    "    print(f\"\\nAverage episode length over {num_episodes} episodes: {average_length:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo episodes were run.\")\n",
    "\n",
    "env.close() # Close the environment when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
